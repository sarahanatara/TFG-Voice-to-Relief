{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76fd16e8",
   "metadata": {},
   "source": [
    "# # Análisis de Resultados del Modelo\n",
    "# \n",
    "# Este notebook analiza los resultados del entrenamiento y evaluación del modelo de transcripción de emergencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1fac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "from jiwer import wer\n",
    "from src.evaluation.metrics import calculate_detailed_metrics\n",
    "from src.utils.config_loader import load_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5252c8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "languages_config = load_config(\"languages\")\n",
    "training_config = load_config(\"training_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d98d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar resultados de evaluación\n",
    "try:\n",
    "    with open('evaluation_results.json', 'r') as f:\n",
    "        results = json.load(f)\n",
    "    print(\"Resultados de evaluación cargados\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Primero ejecuta la evaluación completa\")\n",
    "    results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e97d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar logs de entrenamiento si existen\n",
    "try:\n",
    "    # Asumiendo que usas TensorBoard o guardas logs en JSON\n",
    "    training_logs = pd.read_csv('training_logs.csv')\n",
    "    print(\"Logs de entrenamiento cargados\")\n",
    "except FileNotFound:\n",
    "    training_logs = pd.DataFrame()\n",
    "    print(\"No se encontraron logs de entrenamiento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873783a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar progreso del entrenamiento\n",
    "if not training_logs.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Pérdida de entrenamiento\n",
    "    if 'loss' in training_logs.columns:\n",
    "        axes[0].plot(training_logs['step'], training_logs['loss'])\n",
    "        axes[0].set_title('Pérdida de Entrenamiento')\n",
    "        axes[0].set_xlabel('Step')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].grid(True)\n",
    "    \n",
    "    # WER de validación\n",
    "    if 'eval_wer' in training_logs.columns:\n",
    "        axes[1].plot(training_logs['step'], training_logs['eval_wer'])\n",
    "        axes[1].set_title('WER de Validación')\n",
    "        axes[1].set_xlabel('Step')\n",
    "        axes[1].set_ylabel('WER')\n",
    "        axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd71c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de resultados por idioma\n",
    "if results and 'by_language' in results:\n",
    "    language_results = results['by_language']\n",
    "    \n",
    "    languages = list(language_results.keys())\n",
    "    wers = [language_results[lang]['wer'] for lang in languages]\n",
    "    samples = [language_results[lang]['samples'] for lang in languages]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # WER por idioma\n",
    "    bars = axes[0].bar(languages, wers, color='skyblue', alpha=0.7)\n",
    "    axes[0].set_title('WER por Idioma')\n",
    "    axes[0].set_ylabel('Word Error Rate (WER)')\n",
    "    axes[0].set_ylim(0, max(wers) * 1.1)\n",
    "    \n",
    "    # Añadir valores en las barras\n",
    "    for bar, wer_val in zip(bars, wers):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{wer_val:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Muestras por idioma\n",
    "    axes[1].bar(languages, samples, color='lightcoral', alpha=0.7)\n",
    "    axes[1].set_title('Muestras por Idioma')\n",
    "    axes[1].set_ylabel('Número de Muestras')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis por nivel de ruido\n",
    "if results and 'by_noise_level' in results:\n",
    "    noise_results = results['by_noise_level']\n",
    "    \n",
    "    noise_levels = list(noise_results.keys())\n",
    "    wers_noise = [noise_results[level]['wer'] for level in noise_levels]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(noise_levels, wers_noise, color=['green', 'yellow', 'orange', 'red', 'darkred'])\n",
    "    plt.title('WER por Nivel de Ruido')\n",
    "    plt.ylabel('Word Error Rate (WER)')\n",
    "    plt.xlabel('Nivel de Ruido')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Añadir valores\n",
    "    for bar, wer_val in zip(bars, wers_noise):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{wer_val:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc1ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de frases de emergencia\n",
    "if results and 'by_emergency_phrase' in results:\n",
    "    phrase_results = results['by_emergency_phrase']\n",
    "    \n",
    "    if phrase_results:\n",
    "        phrases = list(phrase_results.keys())\n",
    "        phrase_wers = list(phrase_results.values())\n",
    "        \n",
    "        # Ordenar por WER\n",
    "        sorted_indices = np.argsort(phrase_wers)\n",
    "        sorted_phrases = [phrases[i] for i in sorted_indices]\n",
    "        sorted_wers = [phrase_wers[i] for i in sorted_indices]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        bars = plt.barh(sorted_phrases, sorted_wers, color='lightblue')\n",
    "        plt.title('WER por Frase de Emergencia')\n",
    "        plt.xlabel('Word Error Rate (WER)')\n",
    "        plt.ylabel('Frase de Emergencia')\n",
    "        \n",
    "        # Añadir valores\n",
    "        for bar, wer_val in zip(bars, sorted_wers):\n",
    "            plt.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{wer_val:.3f}', ha='left', va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fdaa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión entre idiomas (ejemplo simplificado)\n",
    "def plot_language_confusion(transcriptions, predictions, languages):\n",
    "    \"\"\"Visualizar confusión entre idiomas\"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import itertools\n",
    "    \n",
    "    # Detectar idioma basado en caracteres (simplificado)\n",
    "    def detect_language(text):\n",
    "        text_lower = text.lower()\n",
    "        for lang, config in languages_config['languages'].items():\n",
    "            for phrase in config.get('emergency_phrases', []):\n",
    "                if phrase.lower() in text_lower:\n",
    "                    return lang\n",
    "        return 'unknown'\n",
    "    \n",
    "    true_langs = [detect_language(text) for text in transcriptions]\n",
    "    pred_langs = [detect_language(text) for text in predictions]\n",
    "    \n",
    "    # Filtrar unknown\n",
    "    valid_indices = [i for i, (t, p) in enumerate(zip(true_langs, pred_langs)) \n",
    "                     if t != 'unknown' and p != 'unknown']\n",
    "    \n",
    "    if valid_indices:\n",
    "        true_langs_filtered = [true_langs[i] for i in valid_indices]\n",
    "        pred_langs_filtered = [pred_langs[i] for i in valid_indices]\n",
    "        \n",
    "        cm = confusion_matrix(true_langs_filtered, pred_langs_filtered, \n",
    "                            labels=list(languages_config['languages'].keys()))\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=languages_config['languages'].keys(),\n",
    "                   yticklabels=languages_config['languages'].keys())\n",
    "        plt.title('Matriz de Confusión entre Idiomas')\n",
    "        plt.xlabel('Predicho')\n",
    "        plt.ylabel('Real')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de errores comunes\n",
    "def analyze_error_patterns(transcriptions, predictions, n_common=10):\n",
    "    \"\"\"Analizar patrones de errores comunes\"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    errors = []\n",
    "    for true, pred in zip(transcriptions, predictions):\n",
    "        if true != pred:\n",
    "            errors.append({\n",
    "                'true': true,\n",
    "                'pred': pred,\n",
    "                'length_diff': abs(len(true) - len(pred))\n",
    "            })\n",
    "    \n",
    "    # Errores más comunes por longitud\n",
    "    if errors:\n",
    "        error_df = pd.DataFrame(errors)\n",
    "        \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        error_df['length_diff'].hist(bins=20, alpha=0.7)\n",
    "        plt.title('Distribución de Diferencias de Longitud')\n",
    "        plt.xlabel('Diferencia de Longitud')\n",
    "        plt.ylabel('Frecuencia')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        # Palabras más comúnmente mal transcritas (simplificado)\n",
    "        all_words = ' '.join([e['true'] for e in errors]).split()\n",
    "        word_counts = Counter(all_words).most_common(n_common)\n",
    "        words, counts = zip(*word_counts)\n",
    "        \n",
    "        plt.bar(range(len(words)), counts)\n",
    "        plt.title(f'Top {n_common} Palabras en Errores')\n",
    "        plt.xlabel('Palabra')\n",
    "        plt.ylabel('Frecuencia')\n",
    "        plt.xticks(range(len(words)), words, rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return error_df\n",
    "    \n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar ejemplos de test para análisis detallado\n",
    "try:\n",
    "    test_df = pd.read_csv('data/generated/test_metadata.csv')\n",
    "    print(f\"Dataset de test: {len(test_df)} muestras\")\n",
    "    \n",
    "    # Ejemplo de análisis con un subconjunto\n",
    "    sample_df = test_df.sample(min(50, len(test_df)))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"No se encontró dataset de test\")\n",
    "    test_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar reporte final de rendimiento\n",
    "def generate_performance_report(results):\n",
    "    \"\"\"Generar reporte completo de rendimiento\"\"\"\n",
    "    report = {\n",
    "        'overall_performance': results.get('overall', {}),\n",
    "        'language_performance': {},\n",
    "        'robustness_analysis': {},\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Análisis por idioma\n",
    "    if 'by_language' in results:\n",
    "        lang_results = results['by_language']\n",
    "        best_lang = min(lang_results.items(), key=lambda x: x[1]['wer'])\n",
    "        worst_lang = max(lang_results.items(), key=lambda x: x[1]['wer'])\n",
    "        \n",
    "        report['language_performance'] = {\n",
    "            'best_performing': {'language': best_lang[0], 'wer': best_lang[1]['wer']},\n",
    "            'worst_performing': {'language': worst_lang[0], 'wer': worst_lang[1]['wer']},\n",
    "            'performance_gap': worst_lang[1]['wer'] - best_lang[1]['wer']\n",
    "        }\n",
    "    \n",
    "    # Análisis de robustez\n",
    "    if 'by_noise_level' in results:\n",
    "        noise_results = results['by_noise_level']\n",
    "        clean_wer = noise_results.get('clean', {}).get('wer', 1.0)\n",
    "        worst_noise_wer = max([r['wer'] for r in noise_results.values()])\n",
    "        \n",
    "        report['robustness_analysis'] = {\n",
    "            'clean_performance': clean_wer,\n",
    "            'worst_case_performance': worst_noise_wer,\n",
    "            'performance_degradation': worst_noise_wer - clean_wer\n",
    "        }\n",
    "    \n",
    "    # Recomendaciones\n",
    "    overall_wer = results.get('overall', {}).get('wer', 1.0)\n",
    "    if overall_wer < 0.1:\n",
    "        report['recommendations'].append(\"✅ Excelente rendimiento general\")\n",
    "    elif overall_wer < 0.2:\n",
    "        report['recommendations'].append(\"⚠️ Buen rendimiento, considerar mejoras en idiomas problemáticos\")\n",
    "    else:\n",
    "        report['recommendations'].append(\"❌ Rendimiento necesita mejora, considerar más datos de entrenamiento\")\n",
    "    \n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f63f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar y mostrar reporte\n",
    "if results:\n",
    "    performance_report = generate_performance_report(results)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"REPORTE FINAL DE RENDIMIENTO\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"\\nRendimiento General: WER = {performance_report['overall_performance'].get('wer', 'N/A'):.3f}\")\n",
    "    \n",
    "    print(f\"\\nAnálisis por Idioma:\")\n",
    "    lang_perf = performance_report['language_performance']\n",
    "    if lang_perf:\n",
    "        print(f\"  Mejor: {lang_perf['best_performing']['language']} (WER: {lang_perf['best_performing']['wer']:.3f})\")\n",
    "        print(f\"  Peor: {lang_perf['worst_performing']['language']} (WER: {lang_perf['worst_performing']['wer']:.3f})\")\n",
    "    \n",
    "    print(f\"\\nAnálisis de Robustez:\")\n",
    "    robustness = performance_report['robustness_analysis']\n",
    "    if robustness:\n",
    "        print(f\"  Condiciones limpias: WER = {robustness['clean_performance']:.3f}\")\n",
    "        print(f\"  Peor caso: WER = {robustness['worst_case_performance']:.3f}\")\n",
    "        print(f\"  Degradación: {robustness['performance_degradation']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nRecomendaciones:\")\n",
    "    for rec in performance_report['recommendations']:\n",
    "        print(f\"  {rec}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975eb785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar análisis completo\n",
    "if results:\n",
    "    analysis_output = {\n",
    "        'performance_report': generate_performance_report(results),\n",
    "        'detailed_results': results,\n",
    "        'visualizations': {\n",
    "            'language_analysis': True,\n",
    "            'noise_analysis': True,\n",
    "            'emergency_phrases_analysis': 'by_emergency_phrase' in results\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('results_analysis.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(analysis_output, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"\\nAnálisis guardado en results_analysis.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
